{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fengSNSWl1X"
   },
   "source": [
    "# Assignment 1. Traffic volume prediction.\n",
    "by Anvar Kurmukov,\n",
    "updated by Bogdan Kirillov, Hekmat Taherinejad, Satyarth Mishra Sharma\n",
    "\n",
    "---\n",
    "\n",
    "By the end of this task you will be able to manipulate huge tabular data:\n",
    "1. Compute different column's statistics (min, max, mean, quantiles etc.);\n",
    "2. Select observations/features by condition/index;\n",
    "3. Create new non-linear combinations of the columns (feature engineering);\n",
    "4. Perform automated data cleaning;\n",
    "\n",
    "and more.\n",
    "\n",
    "---\n",
    "\n",
    "For those who are not familiar with `pandas` we recommend these (alternative) tutorials:\n",
    "\n",
    "1. Single notebook, covers basic pandas functionality (starting with renaming columns ending with using map, apply etc) ~ 30 short examples with links on videos https://nbviewer.jupyter.org/github/justmarkham/pandas-videos/blob/master/pandas.ipynb . Highly recommended for everyone. (about 1-3 hours to go through)\n",
    "\n",
    "2. https://github.com/guipsamora/pandas_exercises/ 11 topics covering all essential functionality with excersises (with solutions).\n",
    "\n",
    "This task will be an easy ride after these tutorials.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBv4L3BoWtY8"
   },
   "source": [
    "We are using a public dataset compiling weather information and traffic data continuously monitored in the Twin Cities, Minnesota from 2012 to 2018. The dataset page can be found [here](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume). We've slightly modified it so please download the dataset provided on Canvas.  \n",
    "\n",
    "You need to download `Metro_Interstate_Traffic_Volume.csv` and place it in the same directory as this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rYmsZ_BQWx6a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mp5vQW5Xtw5"
   },
   "source": [
    "# 1. Loading data\n",
    "\n",
    "As always in Data Science you are starting with making nice cup of tea (or coffee). Your next move is to load the data:\n",
    "\n",
    "- Start with loading `Metro_Interstate_Traffic_Volume.csv` file using `pd.read_csv()` function.\n",
    "- You may also want to increase maximal displayed pandas columns: set `pd.options.display.max_columns` to 30\n",
    "- Print top 10 observations in the table. `.head()`\n",
    "- Print last 10 observations in the table. `.tail()`\n",
    "- Print all the data columns names using method `.columns`\n",
    "- Print data size (number of rows and columns). This is the `.shape` of the data.\n",
    "\n",
    "*Almost* every python has a `head` and a `tail` just as DataFrames do.\n",
    "\n",
    "If you are using Google Colab, you can upload the file in the cell below. If you are NOT using Colab, set COLAB_P in the cell below to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "KS_2J7IXX9ZS",
    "outputId": "6a5a75dd-4245-4992-8c2b-20275dceb5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place your file to the same directory as the notebook, then read your file with pd.read_csv()\n"
     ]
    }
   ],
   "source": [
    "COLAB_P = False\n",
    "if COLAB_P:\n",
    "    print(\"Upload your file, then read it with pd.read_csv()\")\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    fn = list(uploaded.keys())[0]\n",
    "    print(\"File is uploaded to \", fn)\n",
    "else:\n",
    "    print(\"Place your file to the same directory as the notebook, then read your file with pd.read_csv()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IMsqdgKrYugR"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('C://Users/Nikolay/Downloads/Metro_Interstate_Traffic_Volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "H0P12NdQZPxw",
    "outputId": "dc794a4e-c50f-408e-a284-a130a70d2dda"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>date_time</th>\n",
       "      <th>traffic_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>288.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>scattered clouds</td>\n",
       "      <td>2012-10-02 09:00:00</td>\n",
       "      <td>5545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>289.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>2012-10-02 10:00:00</td>\n",
       "      <td>4516.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>289.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>2012-10-02 11:00:00</td>\n",
       "      <td>4767.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>290.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>2012-10-02 12:00:00</td>\n",
       "      <td>5026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>291.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>2012-10-02 13:00:00</td>\n",
       "      <td>4918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>291.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>2012-10-02 14:00:00</td>\n",
       "      <td>5181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>293.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>2012-10-02 15:00:00</td>\n",
       "      <td>5584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>293.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>2012-10-02 16:00:00</td>\n",
       "      <td>6015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>294.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>2012-10-02 17:00:00</td>\n",
       "      <td>5791.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>293.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>2012-10-02 18:00:00</td>\n",
       "      <td>4770.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  holiday    temp  rain_1h  snow_1h  clouds_all weather_main  \\\n",
       "0    None  288.28      0.0      0.0        40.0       Clouds   \n",
       "1    None  289.36      0.0      0.0        75.0       Clouds   \n",
       "2    None  289.58      0.0      0.0        90.0       Clouds   \n",
       "3    None  290.13      0.0      0.0        90.0       Clouds   \n",
       "4    None  291.14      0.0      0.0        75.0       Clouds   \n",
       "5    None  291.72      0.0      0.0         1.0        Clear   \n",
       "6    None  293.17      0.0      0.0         1.0        Clear   \n",
       "7    None  293.86      0.0      0.0         1.0        Clear   \n",
       "8    None  294.14      0.0      0.0        20.0       Clouds   \n",
       "9    None  293.10      0.0      0.0        20.0       Clouds   \n",
       "\n",
       "  weather_description            date_time  traffic_volume  \n",
       "0    scattered clouds  2012-10-02 09:00:00          5545.0  \n",
       "1       broken clouds  2012-10-02 10:00:00          4516.0  \n",
       "2     overcast clouds  2012-10-02 11:00:00          4767.0  \n",
       "3     overcast clouds  2012-10-02 12:00:00          5026.0  \n",
       "4       broken clouds  2012-10-02 13:00:00          4918.0  \n",
       "5        sky is clear  2012-10-02 14:00:00          5181.0  \n",
       "6        sky is clear  2012-10-02 15:00:00          5584.0  \n",
       "7        sky is clear  2012-10-02 16:00:00          6015.0  \n",
       "8          few clouds  2012-10-02 17:00:00          5791.0  \n",
       "9          few clouds  2012-10-02 18:00:00          4770.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe top 10 observations (int)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "MXEj8wifdVjV",
    "outputId": "2c48f70b-b389-49bb-a9f3-3a506cbaa51f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>date_time</th>\n",
       "      <th>traffic_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48194</th>\n",
       "      <td>None</td>\n",
       "      <td>283.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>proximity shower rain</td>\n",
       "      <td>2018-09-30 15:00:00</td>\n",
       "      <td>4302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48195</th>\n",
       "      <td>None</td>\n",
       "      <td>283.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Drizzle</td>\n",
       "      <td>light intensity drizzle</td>\n",
       "      <td>2018-09-30 15:00:00</td>\n",
       "      <td>4302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48196</th>\n",
       "      <td>None</td>\n",
       "      <td>284.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>2018-09-30 16:00:00</td>\n",
       "      <td>4283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48197</th>\n",
       "      <td>None</td>\n",
       "      <td>284.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>2018-09-30 17:00:00</td>\n",
       "      <td>4132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48198</th>\n",
       "      <td>None</td>\n",
       "      <td>284.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Rain</td>\n",
       "      <td>light rain</td>\n",
       "      <td>2018-09-30 18:00:00</td>\n",
       "      <td>3947.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48199</th>\n",
       "      <td>None</td>\n",
       "      <td>283.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>2018-09-30 19:00:00</td>\n",
       "      <td>3543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48200</th>\n",
       "      <td>None</td>\n",
       "      <td>282.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>2018-09-30 20:00:00</td>\n",
       "      <td>2781.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48201</th>\n",
       "      <td>None</td>\n",
       "      <td>282.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Thunderstorm</td>\n",
       "      <td>proximity thunderstorm</td>\n",
       "      <td>2018-09-30 21:00:00</td>\n",
       "      <td>2159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48202</th>\n",
       "      <td>None</td>\n",
       "      <td>282.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>2018-09-30 22:00:00</td>\n",
       "      <td>1450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48203</th>\n",
       "      <td>None</td>\n",
       "      <td>282.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>overcast clouds</td>\n",
       "      <td>2018-09-30 23:00:00</td>\n",
       "      <td>954.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      holiday    temp  rain_1h  snow_1h  clouds_all  weather_main  \\\n",
       "48194    None  283.84     0.00      0.0        75.0          Rain   \n",
       "48195    None  283.84     0.00      0.0        75.0       Drizzle   \n",
       "48196    None  284.38     0.00      0.0        75.0          Rain   \n",
       "48197    None  284.79     0.00      0.0        75.0        Clouds   \n",
       "48198    None  284.20     0.25      0.0        75.0          Rain   \n",
       "48199    None  283.45     0.00      0.0        75.0        Clouds   \n",
       "48200    None  282.76     0.00      0.0        90.0        Clouds   \n",
       "48201    None  282.73     0.00      0.0        90.0  Thunderstorm   \n",
       "48202    None  282.09     0.00      0.0        90.0        Clouds   \n",
       "48203    None  282.12     0.00      0.0        90.0        Clouds   \n",
       "\n",
       "           weather_description            date_time  traffic_volume  \n",
       "48194    proximity shower rain  2018-09-30 15:00:00          4302.0  \n",
       "48195  light intensity drizzle  2018-09-30 15:00:00          4302.0  \n",
       "48196               light rain  2018-09-30 16:00:00          4283.0  \n",
       "48197            broken clouds  2018-09-30 17:00:00          4132.0  \n",
       "48198               light rain  2018-09-30 18:00:00          3947.0  \n",
       "48199            broken clouds  2018-09-30 19:00:00          3543.0  \n",
       "48200          overcast clouds  2018-09-30 20:00:00          2781.0  \n",
       "48201   proximity thunderstorm  2018-09-30 21:00:00          2159.0  \n",
       "48202          overcast clouds  2018-09-30 22:00:00          1450.0  \n",
       "48203          overcast clouds  2018-09-30 23:00:00           954.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe last 10 observations (int)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zui3i6ZOdo2D",
    "outputId": "afbc65c8-ee84-4325-d5c0-9ff307c78f68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['holiday', 'temp', 'rain_1h', 'snow_1h', 'clouds_all', 'weather_main',\n",
       "       'weather_description', 'date_time', 'traffic_volume'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print all the columns/features names (int)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwPaHlDhuklP",
    "outputId": "b49a1252-f5be-4b1c-be5c-d048af3375d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.1 answer is 3 \n",
      "Q1.2 answer is 0       \n",
      "Q1.3 answer is ['temp', 'rain_1h', 'snow_1h', 'clouds_all', 'weather_main', 'weather_description', 'weather_main', 'weather_description'] \n",
      "Q1.4 answer is 2\n"
     ]
    }
   ],
   "source": [
    "# Q1.1 How many columns end with a vowel?\n",
    "# Q1.2 How many columns start with a vowel?\n",
    "# Q1.3 Which columns are associated with the condition of weather?\n",
    "# Q1.4 How many columns have `th` in their names?\n",
    "\n",
    "vowel_start, vowel_end, th_columns, = 0, 0, 0\n",
    "weather_column_list = ['temp', 'rain_1h', 'snow_1h', 'clouds_all', \\\n",
    "                       'weather_main', 'weather_description']\n",
    "for i in df.columns:\n",
    "    if i[-1] in ('a', 'e', 'i', 'o', 'u', 'y'):\n",
    "        vowel_end += 1\n",
    "    if i[0] in ('a', 'e', 'i', 'o', 'u', 'y'):\n",
    "        vowel_start += 1\n",
    "    if i.count('th') > 0:\n",
    "        th_columns += 1\n",
    "    if i.count('weather') > 0:\n",
    "        weather_column_list.append(i)\n",
    "print(f\"Q1.1 answer is {vowel_end} \\nQ1.2 answer is {vowel_start} \\\n",
    "      \\nQ1.3 answer is {weather_column_list} \\nQ1.4 answer is {th_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ka8e4hmZdqLp",
    "outputId": "fefb08cc-d966-465a-ef55-cb5410f50b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433836\n",
      "There are 48204 observations\n",
      "There are 9 features\n"
     ]
    }
   ],
   "source": [
    "# Print data size (int)\n",
    "print(df.size)\n",
    "# Q2.1 How many observations are in the data?\n",
    "print(f\"There are {df.shape[0]} observations\")\n",
    "# Q2.2 How many features are in the data?\n",
    "print(f\"There are {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWqFpMWPdy3E"
   },
   "source": [
    "# 2. Basic data exploration\n",
    "\n",
    "Lets do some basics:\n",
    "\n",
    "`.count()` number of not NaN's in every column.\n",
    "    \n",
    "Is there any missing values in the data?     \n",
    "Count number of unique values in every column .nunique().    \n",
    "What does this tells you about the features, which are most likely categorical and which are most likely numerical?    \n",
    "Use pandas `.describe()` to display basic statistic about the data.   \n",
    "Use pandas `.value_counts()` to count number of unique values in a specific column.   \n",
    "Use pandas `.min()`, `.max()`, `.mean()`, `.std()` to display specific statistics about the data.    \n",
    "Use pandas `.dtypes` field to display data types in columns. \n",
    "Hint You could use `.sort_index()` or `.sort_values()` to sort the result of `.value_counts()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwlcBdvIwlfB",
    "outputId": "1471393c-97aa-437d-ce98-377d5d62556e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of not NaN's in every column: \n",
      "holiday                48204\n",
      "temp                   48203\n",
      "rain_1h                48203\n",
      "snow_1h                48204\n",
      "clouds_all             48201\n",
      "weather_main           48203\n",
      "weather_description    48201\n",
      "date_time              48204\n",
      "traffic_volume         48199\n",
      "dtype: int64\n",
      "Q3.1 NA values in the `clouds_all`:3       \n",
      "Q3.2 NA values are in the `temp`: 1       \n",
      "Q3.3 NA values are in the `rain_1h`: 1       \n",
      "Q3.4 NA values are in the `snow_1h`: 0       \n",
      "Q3.5 explicit NA values in the `traffic_volume`: 5\n"
     ]
    }
   ],
   "source": [
    "# Display number of not NaN's in every column (int)\n",
    "print(f\"number of not NaN's in every column: \\n{df.count()}\")\n",
    "# Q3.1 How many NA values are in the `clouds_all` column?\n",
    "# Q3.2 How many NA values are in the `temp` column?\n",
    "# Q3.3 How many NA values are in the `rain_1h` column?\n",
    "# Q3.4 How many NA values are in the `snow_1h` column?\n",
    "# Q3.5 How many explicit NA values are in the `traffic_volume` column?\n",
    "\n",
    "print(f\"Q3.1 NA values in the `clouds_all`:{df.clouds_all.isna().sum()} \\\n",
    "      \\nQ3.2 NA values are in the `temp`: {df.temp.isna().sum()} \\\n",
    "      \\nQ3.3 NA values are in the `rain_1h`: {df.rain_1h.isna().sum()} \\\n",
    "      \\nQ3.4 NA values are in the `snow_1h`: {df.snow_1h.isna().sum()} \\\n",
    "      \\nQ3.5 explicit NA values in the `traffic_volume`: {df.traffic_volume.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now drop rows with NaN with `.dropna`. Remeber to either reassign your dataframe or provide `inplace=True` argument.\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "bo6KibQ3x4Jw",
    "outputId": "55e2f0a4-7591-4829-dc61-25b645d56487"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>traffic_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48190.000000</td>\n",
       "      <td>48190.000000</td>\n",
       "      <td>48190.000000</td>\n",
       "      <td>48190.000000</td>\n",
       "      <td>48190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.201366</td>\n",
       "      <td>0.334356</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>49.369267</td>\n",
       "      <td>3259.859079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.337406</td>\n",
       "      <td>44.795638</td>\n",
       "      <td>0.008169</td>\n",
       "      <td>39.016127</td>\n",
       "      <td>1986.972809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>272.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1192.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>282.440000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>3380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>291.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>4933.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>310.070000</td>\n",
       "      <td>9831.300000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7280.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               temp       rain_1h       snow_1h    clouds_all  traffic_volume\n",
       "count  48190.000000  48190.000000  48190.000000  48190.000000    48190.000000\n",
       "mean     281.201366      0.334356      0.000222     49.369267     3259.859079\n",
       "std       13.337406     44.795638      0.008169     39.016127     1986.972809\n",
       "min        0.000000      0.000000      0.000000      0.000000        0.000000\n",
       "25%      272.160000      0.000000      0.000000      1.000000     1192.250000\n",
       "50%      282.440000      0.000000      0.000000     64.000000     3380.000000\n",
       "75%      291.800000      0.000000      0.000000     90.000000     4933.000000\n",
       "max      310.070000   9831.300000      0.510000    100.000000     7280.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display basic data statistics using .describe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCxMUIbnx-QD",
    "outputId": "0232e59d-e775-4118-ce07-cb94a96b5b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique values in every column : \n",
      "holiday                   12\n",
      "temp                    5843\n",
      "rain_1h                  372\n",
      "snow_1h                   12\n",
      "clouds_all                60\n",
      "weather_main              11\n",
      "weather_description       38\n",
      "date_time              40562\n",
      "traffic_volume          6704\n",
      "dtype: int64\n",
      "Q4.1 unique values in the `clouds_all`:60       \n",
      "Q4.2 unique values are in the `weather_main`: 11       \n",
      "Q4.3 unique values are in the `weather_description`: 38       \n",
      "Q4.4 unique values are in the `snow_1h`: 12       \n",
      "Q4.5 unique values in the `rain_1h`: 372\n"
     ]
    }
   ],
   "source": [
    "# Count number of unique values in every column (int)\n",
    "print(f\"number of unique values in every column : \\n{df.nunique()}\")\n",
    "\n",
    "# Q4.1 How many unique values are in the `clouds_all` column?\n",
    "# Q4.2 How many unique values are in the `weather_main` column?\n",
    "# Q4.3 How many unique values are in the `weather_description` column?\n",
    "# Q4.4 How many unique values are in the `snow_1h` column?\n",
    "# Q4.5 How many unique values are in the `rain_1h` column?\n",
    "\n",
    "print(f\"Q4.1 unique values in the `clouds_all`:{df.clouds_all.nunique()} \\\n",
    "      \\nQ4.2 unique values are in the `weather_main`: {df.weather_main.nunique()} \\\n",
    "      \\nQ4.3 unique values are in the `weather_description`: {df.weather_description.nunique()} \\\n",
    "      \\nQ4.4 unique values are in the `snow_1h`: {df.snow_1h.nunique()} \\\n",
    "      \\nQ4.5 unique values in the `rain_1h`: {df.rain_1h.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPJMcqYQyD9y",
    "outputId": "d1ba998f-0b2b-469d-c6f0-0304fcb5a3f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5.1 `weather_main` number of occurences : \n",
      "Clouds          15159\n",
      "Clear           13384\n",
      "Mist             5950\n",
      "Rain             5671\n",
      "Snow             2876\n",
      "Drizzle          1821\n",
      "Haze             1359\n",
      "Thunderstorm     1034\n",
      "Fog               912\n",
      "Smoke              20\n",
      "Squall              4\n",
      "Name: weather_main, dtype: int64       \n",
      "Q5.2 `weather_description` number of occurences: \n",
      "sky is clear                           11658\n",
      "mist                                    5950\n",
      "overcast clouds                         5081\n",
      "broken clouds                           4665\n",
      "scattered clouds                        3458\n",
      "light rain                              3371\n",
      "few clouds                              1955\n",
      "light snow                              1946\n",
      "Sky is Clear                            1726\n",
      "moderate rain                           1664\n",
      "haze                                    1359\n",
      "light intensity drizzle                 1100\n",
      "fog                                      912\n",
      "proximity thunderstorm                   673\n",
      "drizzle                                  651\n",
      "heavy snow                               616\n",
      "heavy intensity rain                     467\n",
      "snow                                     293\n",
      "proximity shower rain                    136\n",
      "thunderstorm                             125\n",
      "heavy intensity drizzle                   64\n",
      "thunderstorm with heavy rain              63\n",
      "thunderstorm with light rain              54\n",
      "proximity thunderstorm with rain          52\n",
      "thunderstorm with rain                    37\n",
      "smoke                                     20\n",
      "very heavy rain                           18\n",
      "thunderstorm with light drizzle           15\n",
      "light intensity shower rain               13\n",
      "proximity thunderstorm with drizzle       13\n",
      "light shower snow                         11\n",
      "light rain and snow                        6\n",
      "shower drizzle                             6\n",
      "SQUALLS                                    4\n",
      "sleet                                      3\n",
      "thunderstorm with drizzle                  2\n",
      "freezing rain                              2\n",
      "shower snow                                1\n",
      "Name: weather_description, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count frequency of the values in different columns (list of ints in ascending order)\n",
    "# You could select a column using same syntax as for selecting a key from a dictionary: `data[colname]`\n",
    "# numpy's `unique` function can be useful for this task\n",
    "\n",
    "# Q5.1 For every unique `weather_main` value give its number of occurences.\n",
    "# Q5.2 For every unique `weather_description` value give its number of occurences.\n",
    "\n",
    "print(f\"Q5.1 `weather_main` number of occurences : \\n{df['weather_main'].value_counts()} \\\n",
    "      \\nQ5.2 `weather_description` number of occurences: \\n{df['weather_description'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vIOZuv7yErp",
    "outputId": "11c3fdb7-d4bc-4ad2-f059-d4ac01492e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6.1 max, min, mean and the std of the `traffic_volume` column: \n",
      " (7280.0, 0.0, 3259.859, 1986.973)       \n",
      "Q6.2 max, min, mean and the std of the `clouds_all` column: \n",
      " (100.0, 0.0, 49.369, 39.016)       \n",
      "Q6.3 max, min, mean and the std of the `temp` column`: \n",
      " (310.07, 0.0, 281.201, 13.337)       \n",
      "Q6.4 max, min, mean and the std of the `rain_1h` column: \n",
      " (9831.3, 0.0, 0.334, 44.796)       \n",
      "Q6.5 max, min, mean and the std of the `snow_1h` column: \n",
      " (0.51, 0.0, 0.0, 0.008)\n"
     ]
    }
   ],
   "source": [
    "# Display some column statistics (list of floats, rounded up to 3 digits, e.g. 1.234)\n",
    "\n",
    "# Q6.1 What are the max, min, mean and the std of the `traffic_volume` column?\n",
    "# Q6.2 What are the max, min, mean and the std of the `clouds_all` column?\n",
    "# Q6.3 What are the max, min, mean and the std of the `temp` column?\n",
    "# Q6.4 What are the max, min, mean and the std of the `rain_1h` column?\n",
    "# Q6.5 What are the max, min, mean and the std of the `snow_1h` column?\n",
    "\n",
    "print(f\"Q6.1 max, min, mean and the std of the `traffic_volume` column: \\n {round(df.traffic_volume.max(),3) , round(df.traffic_volume.min(),3), round(df.traffic_volume.mean(),3), round(df.traffic_volume.std(),3)} \\\n",
    "      \\nQ6.2 max, min, mean and the std of the `clouds_all` column: \\n {round(df.clouds_all.max(),3), round(df.clouds_all.min(),3), round(df.clouds_all.mean(),3), round(df.clouds_all.std(),3)} \\\n",
    "      \\nQ6.3 max, min, mean and the std of the `temp` column`: \\n {round(df.temp.max(),3), round(df.temp.min(),3), round(df.temp.mean(),3), round(df.temp.std(),3)} \\\n",
    "      \\nQ6.4 max, min, mean and the std of the `rain_1h` column: \\n {round(df.rain_1h.max(),3), round(df.rain_1h.min(),3), round(df.rain_1h.mean(),3), round(df.rain_1h.std(),3)} \\\n",
    "      \\nQ6.5 max, min, mean and the std of the `snow_1h` column: \\n {round(df.snow_1h.max(),3), round(df.snow_1h.min(),3), round(df.snow_1h.mean(),3), round(df.snow_1h.std(),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71Kx1GnoyG7v",
    "outputId": "4748f2d3-ca71-45e7-8438-728672463e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data types of all columns \n",
      "holiday                 object\n",
      "temp                   float64\n",
      "rain_1h                float64\n",
      "snow_1h                float64\n",
      "clouds_all             float64\n",
      "weather_main            object\n",
      "weather_description     object\n",
      "date_time               object\n",
      "traffic_volume         float64\n",
      "dtype: object\n",
      "4 columns have `object` data type         \n",
      "0 columns have `int64` data type         \n",
      "5 columns have `float64` data type         \n",
      "['temp', 'rain_1h', 'snow_1h', 'clouds_all', 'traffic_volume'] columns with dtype `float64`         \n",
      "[] columns with dtype `int64`\n"
     ]
    }
   ],
   "source": [
    "# Display data types of all columns (int)\n",
    "print(f\"data types of all columns \\n{df.dtypes}\")\n",
    "int64_list, float64_list, object_list = [], [], []\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'int64':\n",
    "        int64_list.append(i)\n",
    "    if df[i].dtype == 'float64':\n",
    "        float64_list.append(i)\n",
    "    if df[i].dtype == 'O':\n",
    "        object_list.append(i)\n",
    "    \n",
    "# Q7.1 How many columns have `object` data type?\n",
    "# Q7.2 How many columns have `int64` data type?\n",
    "# Q7.3 How many columns have `float64` data type?\n",
    "# Q7.4 What are the columns with dtype == `float64`?\n",
    "# Q7.5 What are the columns with dtype == `int64`?\n",
    "print(f'{len(object_list)} columns have `object` data type \\\n",
    "        \\n{len(int64_list)} columns have `int64` data type \\\n",
    "        \\n{len(float64_list)} columns have `float64` data type \\\n",
    "        \\n{float64_list} columns with dtype `float64` \\\n",
    "        \\n{int64_list} columns with dtype `int64`')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rrG2dQQe5yf"
   },
   "source": [
    "# 3. Data selection\n",
    "\n",
    "In pandas.DataFrame you could select\n",
    "\n",
    "  Row/s by position (integer number [0 .. number of rows - 1]) .iloc or by DataFrame.index .loc:   \n",
    "\n",
    "```\n",
    "  data.loc[0]  \n",
    "  data.loc[5:10]  \n",
    "  data.iloc[0]  \n",
    "  data.iloc[5:10]   \n",
    "```\n",
    "\n",
    "Though, this is probably the worst way to manipulate rows.   \n",
    "  Columns by name\n",
    "\n",
    "```\n",
    "  data[columname]\n",
    "```\n",
    "\n",
    "  Row/s and columns\n",
    "\n",
    "```\n",
    "  data.loc[10, columname]  \n",
    "  data.iloc[10, columname]  \n",
    "```\n",
    "\n",
    "Using boolean mask\n",
    "\n",
    "```\n",
    "  mask = data[columname] > value  \n",
    "  data[mask]  \n",
    "```\n",
    "\n",
    "You could combine multiple conditions using & or | (and, or)   \n",
    "\n",
    "```\n",
    "cond1 = data[columname1] > value1  \n",
    "cond2 = data[columname2] > value2  \n",
    "data[cond1 & cond2]  \n",
    "```\n",
    "\n",
    "Using queries .query():  \n",
    "\n",
    "```\n",
    "value = 5 \n",
    "data.query(\"columname > value\")  \n",
    "```\n",
    "\n",
    "You could combine multiple conditions using and, or  \n",
    "\n",
    "```\n",
    "data.query(\"(columname1 > value1) and (columname2 > value2)\")\n",
    "```\n",
    "\n",
    "and others. See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html for more examples.\n",
    "\n",
    "Remember to use different quotation marks \" or ' for columnname inside a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJnCqUx-0YRx",
    "outputId": "0be7619b-9aa3-417c-a1b0-eb4579164d28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q8.1: 278.5        \n",
      "Q8.2: overcast clouds        \n",
      "Q8.3: 1.0        \n",
      "Q8.4: Clouds        \n",
      "Q8.5: 2013-01-06 14:00:00\n"
     ]
    }
   ],
   "source": [
    "# Select rows by position (int) \n",
    "\n",
    "# Q8.1 What is the temperature of the time slot with index 777?\n",
    "# Q8.2 What is the weather description of the time slot with index 999?\n",
    "# Q8.3 How much is cloud coverage with index 1337?\n",
    "# Q8.4 What is the weather main of the time slot with index 314?\n",
    "# Q8.5 When was the time slot with index of 2718 observed?\n",
    "\n",
    "print(f'Q8.1: {df.temp.iloc[777]}\\\n",
    "        \\nQ8.2: {df.weather_description.iloc[999]}\\\n",
    "        \\nQ8.3: {df.clouds_all.iloc[1337]}\\\n",
    "        \\nQ8.4: {df.weather_main.iloc[314]}\\\n",
    "        \\nQ8.5: {df.date_time.iloc[2718]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv5jRoeU0aQG",
    "outputId": "635a0c74-7039-40cc-a391-5b31c8823419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q9.1: 280.43        \n",
      "Q9.2: overcast clouds        \n",
      "Q9.3: 20.0        \n",
      "Q9.4: Clear        \n",
      "Q9.5: 2012-10-04 03:00:00\n"
     ]
    }
   ],
   "source": [
    "# Select rows by index (int)\n",
    "\n",
    "# Q9.1 What is the temperature of the time slot on index 1102?\n",
    "# Q9.2 What is the weather description of the time slot on index 5695?\n",
    "# Q9.3 How much is cloud coverage on the index 1045?\n",
    "# Q9.4 What is the weather main of the time slot from index 252?\n",
    "# Q9.5 When was the time slot with index of 38 captured?\n",
    "\n",
    "print(f'Q9.1: {df.temp.loc[1102]}\\\n",
    "        \\nQ9.2: {df.weather_description.loc[5695]}\\\n",
    "        \\nQ9.3: {df.clouds_all.loc[1045]}\\\n",
    "        \\nQ9.4: {df.weather_main.loc[252]}\\\n",
    "        \\nQ9.5: {df.date_time.loc[38]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZneqD6P0e5C",
    "outputId": "e0a1552e-7042-4ead-eb0a-75cf9a213ea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q10.1: 9308         \n",
      "Q10.2: 2012-10-10 07:00:00         \n",
      "Q10.3: 18036        \n",
      "Q10.4: 912        \n",
      "Q10.5: 2018-04-15 18:00:00\n"
     ]
    }
   ],
   "source": [
    "# Using mask or .query syntax select rows/columns (int)\n",
    "\n",
    "# Q10.1 How many time slots have less than 270 temperature?\n",
    "# Q10.2 When was the first \"light intensity drizzle\" in weather description captured?\n",
    "# Q10.3 How many time slots have cloud coverage more than 75?\n",
    "# Q10.4 How many time slots are foggy? (weather_main = Fog)\n",
    "# Q10.5 When was the last observed timeslot with weather_description \"heavy snow\"?\n",
    "\n",
    "first = df.query(\"weather_description == 'light intensity drizzle'\")[['date_time']].iloc[0][0]\n",
    "Fog = df.query('weather_main == \"Fog\"').shape[0]\n",
    "heavy_snow = df.query('weather_description == \"heavy snow\"')[['date_time']].sort_values(by=['date_time']).iloc[-1][0]\n",
    "\n",
    "print(f\"Q10.1: {df.query('temp < 270').shape[0]} \\\n",
    "        \\nQ10.2: {first} \\\n",
    "        \\nQ10.3: {df.query('clouds_all > 75').shape[0]}\\\n",
    "        \\nQ10.4: {Fog}\\\n",
    "        \\nQ10.5: {heavy_snow}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q11.1: 2070.0        \n",
      "Q11.2: 0.25        \n",
      "Q11.3: 68.0        \n",
      "Q11.4: 3833.0        \n",
      "Q11.5: proximity thunderstorm with rain\n"
     ]
    }
   ],
   "source": [
    "# Q11.1 What is the traffic volume of November 20th 2016, at 20:00?\n",
    "# Q11.2 What is the amount of rain in the 70th rainy time slot (non-zero rain) of the dataset?\n",
    "# Q11.3 How much cloud coverage percentage were in sky on October 16th 2012 at 19:00?\n",
    "# Q11.4 What is the `traffic_volume` of a thirty fourth sample with `clouds_all` == 90?\n",
    "# Q11.5 What is the \"weather_description\" in the 20th \"weather_main\" with Thunderstorm?\n",
    "\n",
    "traf_nov = df.query('date_time == \"2016-11-20 20:00:00\"')[['traffic_volume']].iloc[0][0]\n",
    "rain_am = df.query('rain_1h != 0.0').iloc[69].rain_1h\n",
    "sky_cov_oct = df.query('date_time == \"2012-10-16 19:00:00\"')[[\"clouds_all\"]].iloc[0][0]\n",
    "traf_vol = df.query('clouds_all == 90').iloc[34].traffic_volume\n",
    "weat_des = df.query('weather_main == \"Thunderstorm\"').iloc[20].weather_description\n",
    "\n",
    "print(f'Q11.1: {traf_nov}\\\n",
    "        \\nQ11.2: {rain_am}\\\n",
    "        \\nQ11.3: {sky_cov_oct}\\\n",
    "        \\nQ11.4: {traf_vol}\\\n",
    "        \\nQ11.5: {weat_des}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q12.1: 290.97        \n",
      "Q12.2: 762.0        \n",
      "Q12.3: 289.38        \n",
      "Q12.4: 290.84        \n",
      "Q12.5: 277.06\n"
     ]
    }
   ],
   "source": [
    "# Q12.1 What is the temperature of the tenth holiday?\n",
    "# Q12.2 What is the traffic volume for 99-th time slot with cloud coverage 75 percent?\n",
    "# Q12.3 How much is the temperature of the twelfth holiday?\n",
    "# Q12.4 How much is the temperature the 666-th time slot with weather_description 'proximity thunderstorm'?\n",
    "# Q12.5 What is the temperature of 1337-th time slot with clear sky (clouds_all <= 20)?\n",
    "\n",
    "temp_10_hol = df.query('holiday != None')['temp'].iloc[10]\n",
    "traf_99 = df.query('clouds_all == 75.0').iloc[99].traffic_volume\n",
    "temp_12_hol = df.query('holiday != None')['temp'].iloc[11]\n",
    "temp_666 = df.query(\"weather_description == 'proximity thunderstorm'\").iloc[666].temp\n",
    "temp_1337 = df.query(\"clouds_all <= 20\").iloc[1337].temp\n",
    "\n",
    "print(f'Q12.1: {temp_10_hol}\\\n",
    "        \\nQ12.2: {traf_99}\\\n",
    "        \\nQ12.3: {temp_12_hol}\\\n",
    "        \\nQ12.4: {temp_666}\\\n",
    "        \\nQ12.5: {temp_1337}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBLabnN2fAPc"
   },
   "source": [
    "# 4. Creating new columns\n",
    "\n",
    "Creating new column of pandas.DataFrame is as easy as:\n",
    "```\n",
    "data['new_awesome_column'] = [] \n",
    "```\n",
    "that's it. But such a column is relatively useless. Typically, you would compute something new based on existing data and save it in a new column. For example one might want to sum a number of existing columns:\n",
    "```\n",
    "data['sum'] = data[col1] + data[col2] + ...\n",
    "```\n",
    "Pandas also provides another powerfull tool: .apply, .map(), .applymap() methods (they are kinda the same, but not quite). https://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas . They allow you to apply some function to every value in the column/s (row-wise) or row (column-wise) or cell (element-wise). For example, same computations of sum using .apply():\n",
    "```\n",
    "data['sum'] = data[[col1, col2, col3]].apply(sum, axis=1)\n",
    "```\n",
    "you are not restricted to existent functions, .apply() accepts any function (including lambda functions):\n",
    "```\n",
    "data['sum'] = data[[col1, col2, col3]].apply(lambda x: x[0]+x[1]+x[2], axis=1)\n",
    "```\n",
    "or ordinary python function (if this it should have complex behaviour):\n",
    "```\n",
    "def _sum(x):\n",
    "    total = 0\n",
    "    for elem in x:\n",
    "        total += elem\n",
    "    return total\n",
    "\n",
    "data['sum'] = data[[col1, col2, col3]].apply(_sum, axis=1) \n",
    "```\n",
    "Many pandas methods has axis parameter axis=0 refers to rows, axis=1 refers to columns.\n",
    "\n",
    "Warning. You should never use for loops to sum numerical elements from the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nxbj9Po2Le18"
   },
   "outputs": [],
   "source": [
    "# Create new columns using the old ones (new column in your DataFrame)\n",
    "\n",
    "# Q13.1 Create a `temp_in_celcius` column from the existing `temp` (kelvin) using any method above\n",
    "# Q13.2 Create a new bool column `hot` which indicates whether the time slot was hot (temp > 300)\n",
    "# Q13.3 Create a new bool column `rainy_and_cloudy` which indicates whether it was rainy (>0.1) AND cloudy (>50)\n",
    "# Q13.4 Create a new bool column `is_holiday` which indicates whether the day of the time slot falls on any holiday\n",
    "# Q13.5 Create a new column `traffic_cat` by splitting a `traffic_volume` into 5 ([1..5]) distinct intervals: 0 < x <=20%,\n",
    "# 20% < x <= 40%, ... 80% < x <= 100% percentiles. You could use `.quantile()` to compute percentiles.\n",
    "\n",
    "\n",
    "# Q13.1 \n",
    "df['temp_in_celcius'] = df['temp'].apply(lambda x: x - 273.15)\n",
    "# Q13.2\n",
    "df['hot'] = df['temp'] > 300\n",
    "# Q13.3\n",
    "df['rainy_and_cloudy'] = (df['rain_1h'] > 0.1) & (df['clouds_all'] > 50.0)\n",
    "# Q13.4\n",
    "df['is_holiday'] = (df['holiday'] != \"None\")\n",
    "# Q13.5\n",
    "df['traffic_cat'] = pd.qcut(df['traffic_volume'],\n",
    "                              q=[0, .2, .4, .6, .8, 1],\n",
    "                              labels=['1', '2', '3', '4', '5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xm4Ve-s70h73",
    "outputId": "d68d003c-151e-4803-9823-5c8d1686fff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q14.1: 643        \n",
      "Q14.2: 978        \n",
      "Q14.3: 12133        \n",
      "Q14.4: 4780.0        \n",
      "Q14.5: 6673.0\n"
     ]
    }
   ],
   "source": [
    "# Using mask or .query syntax select rows/columns (int).\n",
    "# For working with dates, define helper functions that operate on the date_time string.\n",
    "\n",
    "# Q14.1 How many cloudy time slots were captured in autumn 2016? Including both start and end day.\n",
    "# Q14.2 How many rainy time slots that were captured in the fall, with traffic volume more than 2000?\n",
    "# Q14.3 How many time slots that are warmer than 270, have weather main \"Clouds\"?\n",
    "# Q14.4 What is the minimum traffic volume of time slots captured on March 8th (all years), that was warmer than 290?\n",
    "# Q14.5 How much is the maximum traffic volume for the time slots were captured in June 2017 and has clear sky (weather_main)?\n",
    "\n",
    "cloudy_slots = df.query(\"(date_time <= '2016-11-30 23:00:00') and (date_time >= '2016-09-01 00:00:00') \\\n",
    "                        and (weather_main == 'Clouds')\").shape[0]\n",
    "\n",
    "rainy_slots = df.query(\"(((date_time <= '2012-11-30 23:00:00') and (date_time >= '2012-09-01 00:00:00')) \\\n",
    "                        or ((date_time <= '2013-11-30 23:00:00') and (date_time >= '2013-09-01 00:00:00')) \\\n",
    "                        or ((date_time <= '2014-11-30 23:00:00') and (date_time >= '2014-09-01 00:00:00')) \\\n",
    "                        or ((date_time <= '2015-11-30 23:00:00') and (date_time >= '2015-09-01 00:00:00')) \\\n",
    "                        or ((date_time <= '2016-11-30 23:00:00') and (date_time >= '2016-09-01 00:00:00')) \\\n",
    "                        or ((date_time <= '2017-11-30 23:00:00') and (date_time >= '2017-09-01 00:00:00')) \\\n",
    "                        or ((date_time <= '2018-11-30 23:00:00') and (date_time >= '2018-09-01 00:00:00'))) \\\n",
    "                        and (weather_main == 'Rain') and (traffic_volume > 2000.0)\").shape[0]\n",
    "\n",
    "warm_slots = df[(df.temp > 270.0) &  (df.weather_main.str.contains('Clouds'))].shape[0]\n",
    "\n",
    "s = pd.Series(df.query(\"temp > 290\").date_time.str.contains('03-08'))\n",
    "warm_8march = df.loc[s[s].index[0]:s[s].index[-1]].traffic_volume.min()\n",
    "\n",
    "max_traf = df.query(\"(date_time <= '2017-07-01 00:00:00') and (date_time >= '2017-06-01 00:00:00') \\\n",
    "                         and (weather_main == 'Clear')\").traffic_volume.max()\n",
    "\n",
    "print(f'Q14.1: {cloudy_slots}\\\n",
    "        \\nQ14.2: {rainy_slots}\\\n",
    "        \\nQ14.3: {warm_slots}\\\n",
    "        \\nQ14.4: {warm_8march}\\\n",
    "        \\nQ14.5: {max_traf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q15.1: 275.80473877851364        \n",
      "Q15.2: 1462.0        \n",
      "Q15.3: 3740.6666666666665        \n",
      "Q15.4: 282.01        \n",
      "Q15.5: 307.09\n"
     ]
    }
   ],
   "source": [
    "# Using mask or .query syntax select rows/columns and compute simple statistics (float)\n",
    "\n",
    "\n",
    "# Q15.1 What was the average temperature of time slots with main weather \"Haze\"?\n",
    "# Q15.2 What was the traffic volume of the coldest time slot of the year 2016?\n",
    "# Q15.3 What was the traffic volume of the highest amount of snow in one hour?\n",
    "# Q15.4 What is the median of temperatures captured in April 2017?\n",
    "# Q15.5 What is the maximum temperature of time slots with clear sky?\n",
    "\n",
    "av_temp = df.query('weather_main == \"Haze\"').temp.mean()\n",
    "\n",
    "cold_2016 = df.query(\"(date_time >= '2016-01-01 00:00:00') and (date_time < '2017-01-01 00:00:00')\") \\\n",
    "        .sort_values(by=['temp'], ascending = True).iloc[0].traffic_volume\n",
    "\n",
    "max_snow = df.sort_values(by=['snow_1h'], ascending = False).snow_1h.iloc[0]\n",
    "max_snow_traf = df.query('snow_1h == @max_snow').traffic_volume.mean()\n",
    "\n",
    "median_april_temp = df.query(\"(date_time >= '2017-04-01 00:00:00') and (date_time < '2017-05-01 00:00:00')\").temp.median()\n",
    "\n",
    "max_clear_sky_temp = df.query('clouds_all == 0')['temp'].max()\n",
    "\n",
    "print(f'Q15.1: {av_temp}\\\n",
    "        \\nQ15.2: {cold_2016}\\\n",
    "        \\nQ15.3: {max_snow_traf}\\\n",
    "        \\nQ15.4: {median_april_temp}\\\n",
    "        \\nQ15.5: {max_clear_sky_temp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fzig1HmlL4rq",
    "outputId": "9ebe4adf-0811-4acc-85e9-7907180880a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q16.1: 13.584578809434692        \n",
      "Q16.2: 865.4426229508197        \n",
      "Q16.3: 3262.8938270065864        \n",
      "Q16.4: 5870.913350649351        \n",
      "Q16.5: 485.5536195809998\n"
     ]
    }
   ],
   "source": [
    "# Using mask or .query syntax select rows/columns (float)\n",
    "\n",
    "# Q16.1 What is the average temperature in celcius of the time slots with rainy_and_cloudy=True?\n",
    "# Q16.2 What is the average traffic volume on holidays?\n",
    "# Q16.3 What is the average traffic volume on non-holidays?\n",
    "# Q16.4 What is the average traffic volume in the highest quantile?\n",
    "# Q16.5 What is the average traffic volume in the lowest quantile?\n",
    "\n",
    "\n",
    "av_rc = df.query('rainy_and_cloudy == True').temp_in_celcius.mean()\n",
    "av_hol_tr = df.query('is_holiday == True').traffic_volume.mean()\n",
    "av_nonhol_tr = df.query('is_holiday == False').traffic_volume.mean()\n",
    "highest_quantile_traf = df.query('traffic_cat == \"5\"').traffic_volume.mean()\n",
    "lowest_quantile_traf = df.query('traffic_cat == \"1\"').traffic_volume.mean()\n",
    "\n",
    "print(f'Q16.1: {av_rc}\\\n",
    "        \\nQ16.2: {av_hol_tr}\\\n",
    "        \\nQ16.3: {av_nonhol_tr}\\\n",
    "        \\nQ16.4: {highest_quantile_traf}\\\n",
    "        \\nQ16.5: {lowest_quantile_traf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezvSnhPRfFXM"
   },
   "source": [
    "# 5. Basic date processing\n",
    "\n",
    "You figure out that column date is to harsh for you, so you decided to convert it to a more plausible format:\n",
    "\n",
    "- Use pandas method to_datetime() to convert the date to a good format.\n",
    "- Extract year, month, day and weekday from your new date column. Save them to separate columns.\n",
    "- How many columns has your data now?\n",
    "- Drop column date, remember to set inplace parameter to True.\n",
    "\n",
    "Hint: for datetime formatted date you could extract the year as follow:\n",
    "```\n",
    "data.date.dt.year\n",
    "```\n",
    "Very often date could be a ridiculously rich feature, sometimes it is holidays that matters, sometimes weekends, sometimes some special days like black friday.\n",
    "\n",
    "Learn how to work with date in Python!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZghL5CmKTBZc"
   },
   "outputs": [],
   "source": [
    "# Create new columns based on `Captured` column\n",
    "\n",
    "# Q17.1 Convert date to datetime format\n",
    "# Q17.2 Extract and store `year`\n",
    "# Q17.3 Extract and store `month`\n",
    "# Q17.4 Extract and store `day`\n",
    "# Q17.5 Extract and store `weekday` (Monday - 0, Sunday - 6)\n",
    "# Q17.6 Extract and store `hour`\n",
    "\n",
    "\n",
    "# Q17.1\n",
    "df['date_time'] = pd.to_datetime(df['date_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "# Q17.2 \n",
    "df['year'] = df.date_time.dt.year\n",
    "# Q17.3\n",
    "df['month'] = df.date_time.dt.month\n",
    "# Q17.4 \n",
    "df['day'] = df.date_time.dt.day\n",
    "# Q17.5 \n",
    "df['weekday'] =df.date_time.dt.weekday\n",
    "# Q17.6 \n",
    "df['hour'] = df.date_time.dt.hour\n",
    "\n",
    "df.drop(columns=['date_time'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UU_wYcyETK9T",
    "outputId": "4869f3a1-0740-4830-e014-70e1da8ba8e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q18.1: 3         \n",
      "Q18.2: 5         \n",
      "Q18.3: 3303.049334377447        \n",
      "Q18.4: 4749.295714285714        \n",
      "Q18.5: 3445.976\n"
     ]
    }
   ],
   "source": [
    "# Find some date related information from the data (int)\n",
    "\n",
    "# Q18.1 What is the weekday with the highest traffic volume?\n",
    "# Q18.2 What is the weekday with the lowest traffic volume?\n",
    "# Q18.3 What is the average traffic volume during months of September?\n",
    "# Q18.4 What is the average traffic volume in the time period between 15-19 hours\n",
    "# Q18.5 What is the average traffic volume on World Bicycle Day (June 3)?\n",
    "\n",
    "highest_traffic_weekday = int(df.sort_values(by=['traffic_volume'],ascending = False).weekday.iloc[0])\n",
    "\n",
    "lowest_traffic_weekday = int(df.sort_values(by=['traffic_volume'],ascending = True).weekday.iloc[0])\n",
    "\n",
    "\n",
    "aver_sept_traf = df.query('month == 9').traffic_volume.mean()\n",
    "\n",
    "aver_15_19_traf = df.query(\"(hour >= 15) and (hour <= 19)\").traffic_volume.mean()\n",
    "\n",
    "bic_day_traf = df.query(\"(month == 6) and (day == 3)\").traffic_volume.mean()\n",
    "\n",
    "print(f'Q18.1: {highest_traffic_weekday} \\\n",
    "        \\nQ18.2: {lowest_traffic_weekday} \\\n",
    "        \\nQ18.3: {aver_sept_traf}\\\n",
    "        \\nQ18.4: {aver_15_19_traf}\\\n",
    "        \\nQ18.5: {bic_day_traf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LrPCPd0fRYz"
   },
   "source": [
    "# 6. Groupby\n",
    "\n",
    "from the documentation https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html\n",
    "\n",
    "By “group by” we are referring to a process involving one or more of the following steps:\n",
    "\n",
    "- Splitting the data into groups based on some criteria.\n",
    "- Applying a function to each group independently.\n",
    "- Combining the results into a data structure.\n",
    "\n",
    "`.groupby()` is one of the most powerfull tool for feature engineering. Very often it is used to group object with the same categorical characteristics and compute some statistics (e.g. mean, max, etc.) of a their numerical characteric.\n",
    "\n",
    "Instead of computing average traffic volume with for each month you could compute average traffic volumes for every month in a single command:\n",
    "```\n",
    "data.groupby('month')['traffic_volume'].mean()\n",
    "```\n",
    "You could also make multi-column groups:\n",
    "```\n",
    "data.groupby(['weekday','month'])['traffic_volume'].min()\n",
    "```\n",
    "next, you could compute multiple aggregation functions:\n",
    "```\n",
    "data.groupby(['weekday','month'])['traffic_volume'].agg([min, max])\n",
    "```\n",
    "instead of using built-in functions you could compute custom functions using apply:\n",
    "```\n",
    "import numpy as np\n",
    "data.groupby(['weekday','month'])['traffic_volume'].apply(lambda x: np.quantile(x, .5))\n",
    "```\n",
    "and the coolest thing now is that you can map the results of groupby back on your DataFrame!\n",
    "```\n",
    "gp = data.groupby(['month'])['traffic_volume'].median()\n",
    "data['gp_feature'] = data['month'].map(gp)\n",
    "```\n",
    "Now, if some timeslot has month == 2, its gp_feature will be equal to the median traffic volume amongst all observations in February\n",
    "\n",
    "Read more examples in the documentation https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nRmmkfIVXp5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q19.1: year\n",
      "2012    3225.0\n",
      "2013    3344.0\n",
      "2014    3316.0\n",
      "2015    3368.0\n",
      "2016    3258.5\n",
      "2017    3530.0\n",
      "2018    3400.0\n",
      "Name: traffic_volume, dtype: float64        \n",
      "Q19.2: weekday\n",
      "0    3619.0\n",
      "1    4070.0\n",
      "2    4315.0\n",
      "3    4280.0\n",
      "4    4336.5\n",
      "5    3003.0\n",
      "6    2260.0\n",
      "Name: traffic_volume, dtype: float64        \n",
      "Q19.3: traffic_cat\n",
      "1    5.445774\n",
      "2    6.031004\n",
      "3    9.244710\n",
      "4    9.797489\n",
      "5    9.740191\n",
      "Name: temp_in_celcius, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create some groupby features\n",
    "\n",
    "# Q19.1 `traffic_by_year` groupby `year` and compute median traffic volume.\n",
    "# Q19.2 `traffic_by_weekday` groupby `weekday` and compute median traffic volume.\n",
    "# Q19.3 `temperature_by_traffic` groupby `traffic_cat` and compute average temperature in celsius.\n",
    "\n",
    "traffic_by_year = df.groupby(['year']).median().traffic_volume\n",
    "\n",
    "traffic_by_weekday = df.groupby(['weekday']).median().traffic_volume\n",
    "\n",
    "temperature_by_traffic = df.groupby(['traffic_cat']).mean().temp_in_celcius\n",
    "\n",
    "print(f'Q19.1: {traffic_by_year}\\\n",
    "        \\nQ19.2: {traffic_by_weekday}\\\n",
    "        \\nQ19.3: {temperature_by_traffic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Y3IX6U1fOVI"
   },
   "source": [
    "# 7. Building a regression model\n",
    "\n",
    "- You do not need to normalize data for tree models, and for linear/knn models this step is essential.\n",
    "- Remember, that not all of the features in the table are numeric, some of them might be viewed as categorical.\n",
    "- You may create or drop any features you want - try to only keep features which you think will be relevant to the prediction of traffic volume.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AwdbUu2wYKpB"
   },
   "outputs": [],
   "source": [
    "# Q20 Separate your data into inputs and targets, keeping only relevant inputs. Drop any features computed from the output eg. `traffic_cat`\n",
    "Y = df[\"traffic_volume\"].values\n",
    "\n",
    "Xdf = df.drop(columns=['holiday', 'temp', 'traffic_volume','traffic_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf = pd.get_dummies(Xdf, columns=['is_holiday', 'weather_main', 'weather_description','rainy_and_cloudy', 'hot', \\\n",
    "                                  'month', 'year', 'weekday', 'hour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to split our data into train and test sets. Generally a random split is used, but one needs to be very careful with time series data - we need to make sure train and test data don't contain mixed adjacent time slots. In general with time series, it is recommended not to predict values from the past using input information from the future (although the applicability of this rule in our case is debatable), so we'll use sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) class here. TimeSeriesSplit splits data into a number of folds, then only provides data from past folds to train a model tested on the currently considered fold. So if we split our data into five parts, we'll get four folds:\n",
    "\n",
    "1. Train on [0], test on [1]\n",
    "2. Train on [0,1], test on [2]\n",
    "3. Train on [0, 1, 2], test on [3]\n",
    "4. Train on [0, 1, 2, 3], test on [4]\n",
    "\n",
    "For the following tasks, you are required to use train and test indices from the last fold provided by TimeSeriesSplit with `n_splits` = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdLqDhuCYO1Z",
    "outputId": "99703cb2-7dc5-4987-cd29-7f1a149a2b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 8032 8033 8034] TEST: [ 8035  8036  8037 ... 16063 16064 16065]\n",
      "TRAIN: [    0     1     2 ... 16063 16064 16065] TEST: [16066 16067 16068 ... 24094 24095 24096]\n",
      "TRAIN: [    0     1     2 ... 24094 24095 24096] TEST: [24097 24098 24099 ... 32125 32126 32127]\n",
      "TRAIN: [    0     1     2 ... 32125 32126 32127] TEST: [32128 32129 32130 ... 40156 40157 40158]\n",
      "TRAIN: [    0     1     2 ... 40156 40157 40158] TEST: [40159 40160 40161 ... 48187 48188 48189]\n"
     ]
    }
   ],
   "source": [
    "# Q21 Split your data into train and test parts.\n",
    "# How many records (rows) do you have in train and test tables? (list of int)?\n",
    "# Use sklearn.model_selection.TimeSeriesSplit with n_splits=5\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(Xdf):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_train = StandardScaler()\n",
    "scaler_test = StandardScaler()\n",
    "\n",
    "X_train, X_test = Xdf[:40158], Xdf[40158:]\n",
    "y_train, y_test = Y[:40158], Y[40158:]\n",
    "\n",
    "scaler_train.fit(X_train)\n",
    "X_train_scaled = scaler_train.transform(X_train)\n",
    "\n",
    "scaler_test.fit(X_test)\n",
    "X_test_scaled = scaler_test.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqubzK5rYQA9",
    "outputId": "d9853eb1-2a3c-4eb2-e01a-aaceeb6d5b96"
   },
   "outputs": [],
   "source": [
    "# Create a predictive regression model of a traffic volume.\n",
    "\n",
    "# Q22.1 Use linear regression with l2 regularization (Ridge regression)\n",
    "# Q22.2 Use decision tree regression\n",
    "# Q22.3 Use k nearest neighbours regression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#Q22.1\n",
    "Ridge_regression = Ridge(random_state = 17)\n",
    "#Q22.2\n",
    "DecisionTree_Regressor = DecisionTreeRegressor(random_state = 17)\n",
    "#Q23.3\n",
    "KNeighbors_Regressor = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "7uWxRty0YSBd"
   },
   "outputs": [],
   "source": [
    "# Use grid search to select optimal hyperparamters of your models. \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "# Q23.1 Alpha for a ridge regression\n",
    "alpha_range = [0.01, 0.1 , 1.0, 10.0, 50.0, 100.0, 150.0, 200.0, 250.0, 300.0]\n",
    "param_ridge = dict(alpha=alpha_range)\n",
    "grid = GridSearchCV(Ridge_regression, param_ridge, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_ridge = grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Q23.2 Depth for the tree\n",
    "depth_range = list(range(2, 26, 2))\n",
    "param_dt = dict(max_depth=depth_range)\n",
    "grid = GridSearchCV(DecisionTree_Regressor, param_dt, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_dt = grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Q23.3 Number of neighbours for the knn\n",
    "k_range = list(range(1, 5))\n",
    "param_knn = dict(n_neighbors=k_range)\n",
    "grid = GridSearchCV(KNeighbors_Regressor, param_knn, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_knn = grid.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IVihHjlpYTrB",
    "outputId": "755e3cb5-acf6-4022-88a8-188cba883bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_MSE_Ridge: 650381.0500212344,         \n",
      "test_MSE_Ridge: 664501.1772639198\n",
      "train_MSE_Dec_Tree: 195989.24554698684,         \n",
      "test_MSE_Dec_Tree: 367441.15987779055\n",
      "train_MSE_knn: 311828.7329299268,         \n",
      "test_MSE_knn: 640952.9244272908\n"
     ]
    }
   ],
   "source": [
    "# Compute train and test mean squared error for your best models (list of float).\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Q24.1 Train, test MSE using linear regression with l2 regularization\n",
    "#grid_regression.fit(X_train, y_train)\n",
    "reg = grid_ridge.best_estimator_\n",
    "reg.fit(X_train_scaled, y_train)\n",
    "print(f\"train_MSE_Ridge: {mean_squared_error(y_train, reg.predict(X_train_scaled))}, \\\n",
    "        \\ntest_MSE_Ridge: {mean_squared_error(y_test, reg.predict(X_test_scaled))}\")\n",
    "\n",
    "# Q24.2 Train, test MSE using decision tree regression\n",
    "#grid_dt.fit(X_train, y_train)\n",
    "dt = grid_dt.best_estimator_\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "print(f\"train_MSE_Dec_Tree: {mean_squared_error(y_train, dt.predict(X_train_scaled))}, \\\n",
    "        \\ntest_MSE_Dec_Tree: {mean_squared_error(y_test, dt.predict(X_test_scaled))}\")\n",
    "\n",
    "# Q24.3 Train, test MSE using k nearest neighbours regression\n",
    "#grid_knn.fit(X_train, y_train)\n",
    "knn = grid_knn.best_estimator_\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print(f\"train_MSE_knn: {mean_squared_error(y_train, knn.predict(X_train_scaled))}, \\\n",
    "        \\ntest_MSE_knn: {mean_squared_error(y_test, knn.predict(X_test_scaled))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqwSIwVmYVZ6",
    "outputId": "c8bc2903-7def-4076-b7ba-b13cd2235b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_R^2_Ridge: 0.8358406637021398,         \n",
      "test_R^2_Ridge: 0.8286657446938548\n",
      "train_R^2_Dec_Tree: 0.9505313623921526,         \n",
      "test_R^2_Dec_Tree: 0.9052593740229243\n",
      "train_R^2_knn: 0.9212929130780976,         \n",
      "test_R^2_knn: 0.8347374003982693\n"
     ]
    }
   ],
   "source": [
    "# Compute train and test R^2 for your best models (list of float).\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Q25.1 Train, test R^2 using linear regression with l2 regularization\n",
    "print(f\"train_R^2_Ridge: {r2_score(y_train, reg.predict(X_train_scaled))}, \\\n",
    "        \\ntest_R^2_Ridge: {r2_score(y_test, reg.predict(X_test_scaled))}\")\n",
    "\n",
    "# Q25.2 Train, test R^2 using decision tree regression\n",
    "print(f\"train_R^2_Dec_Tree: {r2_score(y_train, dt.predict(X_train_scaled))}, \\\n",
    "        \\ntest_R^2_Dec_Tree: {r2_score(y_test, dt.predict(X_test_scaled))}\")\n",
    "\n",
    "# Q25.3 Train, test R^2 using k nearest neighbours regression\n",
    "print(f\"train_R^2_knn: {r2_score(y_train, knn.predict(X_train_scaled))}, \\\n",
    "        \\ntest_R^2_knn: {r2_score(y_test, knn.predict(X_test_scaled))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "qO3zWCyHYXQo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hour_16</th>\n",
       "      <td>471.905793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_17</th>\n",
       "      <td>400.771231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_15</th>\n",
       "      <td>383.465859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_14</th>\n",
       "      <td>328.265855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_7</th>\n",
       "      <td>301.812676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         coef_importance\n",
       "hour_16       471.905793\n",
       "hour_17       400.771231\n",
       "hour_15       383.465859\n",
       "hour_14       328.265855\n",
       "hour_7        301.812676"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q26 Which features have largest (by absolute value) weight in your linear model (top 5 features)? (list of str).\n",
    "#Ridge Regression\n",
    "pd.DataFrame(reg.coef_, Xdf.columns) \\\n",
    "            .rename(columns={0:'coef_importance'}) \\\n",
    "            .sort_values(by=['coef_importance'],ascending = False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hour_0</th>\n",
       "      <td>-485.087098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_4</th>\n",
       "      <td>-515.324191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_1</th>\n",
       "      <td>-547.145134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_2</th>\n",
       "      <td>-572.958080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_3</th>\n",
       "      <td>-574.773355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        coef_importance\n",
       "hour_0      -485.087098\n",
       "hour_4      -515.324191\n",
       "hour_1      -547.145134\n",
       "hour_2      -572.958080\n",
       "hour_3      -574.773355"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ridge Regression\n",
    "pd.DataFrame(reg.coef_, Xdf.columns) \\\n",
    "            .rename(columns={0:'coef_importance'}) \\\n",
    "            .sort_values(by=['coef_importance'],ascending = False)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hour_4</th>\n",
       "      <td>0.107533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_1</th>\n",
       "      <td>0.107186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_0</th>\n",
       "      <td>0.106322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_2</th>\n",
       "      <td>0.105415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_3</th>\n",
       "      <td>0.097054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature_importance\n",
       "hour_4            0.107533\n",
       "hour_1            0.107186\n",
       "hour_0            0.106322\n",
       "hour_2            0.105415\n",
       "hour_3            0.097054"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dt.feature_importances_, Xdf.columns) \\\n",
    "            .rename(columns={0:'feature_importance'}) \\\n",
    "            .sort_values(by=['feature_importance'],ascending = False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure your .ipynb is linearly executable     \n",
    "# Kernel -> Restart & Run All -> No ERROR cells"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
